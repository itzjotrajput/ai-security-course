# Module 2 â€” Prompt Injection & LLM Security

## Objective
Learn how attackers manipulate AI prompts and how to defend.

---

## Red Team Techniques (Educational)

- Instruction override attempts
- System prompt extraction
- Tool misuse simulation
- Data exfiltration attempts

---

## Blue Team Defenses

- Prompt isolation
- Context boundary enforcement
- Tool allowlists
- Output schema validation
- Response filtering

---

## Lab Idea

Build a simple prompt validation layer:
- Reject suspicious patterns
- Log abnormal prompts
- Enforce JSON output structure
